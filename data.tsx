import React from 'react';
import { Chapter, Language, QuizQuestion } from './types';
import MathBlock from './components/MathBlock';
import InteractiveAttention from './components/InteractiveAttention';
import ArchitectureDiagram from './components/ArchitectureDiagram';
import TokenizationDemo from './components/TokenizationDemo';
import PositionalEncodingViz from './components/PositionalEncodingViz';
import QuizSection from './components/QuizSection';
import CodeBlock from './components/CodeBlock';
import { BarChart, Bar, XAxis, YAxis, CartesianGrid, Tooltip, Legend, ResponsiveContainer } from 'recharts';

const perfData = [
  { name: 'ByteNet', bleu: 23.7, speed: 10 },
  { name: 'Deep-Att', bleu: 24.6, speed: 20 },
  { name: 'GNMT', bleu: 24.6, speed: 50 },
  { name: 'Transformer (Base)', bleu: 27.3, speed: 90 },
  { name: 'Transformer (Big)', bleu: 28.4, speed: 80 },
];

export const uiTranslations = {
  en: {
    home: "Home",
    curriculum: "Curriculum",
    start: "Start Learning",
    paper: "Read Paper",
    prev: "Previous",
    next: "Next",
    title: "Transformer Explained",
    subtitle: "Transformer Decoded",
    desc: "A comprehensive, interactive deep dive into the architecture that revolutionized Natural Language Processing. Based on the paper \"Attention Is All You Need\".",
    whyTitle: "Why this matters",
    whyDesc: "The Transformer abandoned the recurrence of RNNs and introduced a purely attention-based architecture. This shift enabled massive parallelization, paving the way for models like BERT, GPT-4, and Gemini.",
    stats: { published: "Published", citations: "Citations", concept: "Key Concept" }
  },
  zh: {
    home: "é¦–é¡µ",
    curriculum: "è¯¾ç¨‹å¤§çº²",
    start: "å¼€å§‹å­¦ä¹ ",
    paper: "é˜…è¯»è®ºæ–‡",
    prev: "ä¸Šä¸€ç« ",
    next: "ä¸‹ä¸€ç« ",
    title: "Transformer è¯¦è§£",
    subtitle: "Transformer è§£å¯†",
    desc: "æ·±å…¥æµ…å‡ºåœ°è§£ææ”¹å˜è‡ªç„¶è¯­è¨€å¤„ç†æ ¼å±€çš„ Transformer æ¶æ„ã€‚åŸºäºè®ºæ–‡ã€ŠAttention Is All You Needã€‹ã€‚",
    whyTitle: "æ ¸å¿ƒæ„ä¹‰",
    whyDesc: "Transformer æ‘’å¼ƒäº† RNN çš„å¾ªç¯ç»“æ„ï¼Œå¼•å…¥äº†çº¯ç²¹çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™ä¸€è½¬å˜ä¸º BERTã€GPT-4 å’Œ Gemini ç­‰æ¨¡å‹çš„å¤§è§„æ¨¡å¹¶è¡ŒåŒ–é“ºå¹³äº†é“è·¯ã€‚",
    stats: { published: "å‘å¸ƒäº", citations: "å¼•ç”¨æ¬¡æ•°", concept: "æ ¸å¿ƒæ¦‚å¿µ" }
  }
};

// --- DATA DEFINITIONS ---

const chaptersEn: Chapter[] = [
  {
    id: 'ch0',
    title: '0. Inputs & Embeddings',
    description: 'Before the architecture: How machines read text',
    sections: [
      {
        id: '0-1',
        title: 'From Text to Numbers',
        difficulty: 'basic',
        content: (
          <div>
            <p className="mb-4">
              Computers cannot understand raw text like "Hello". They can only process numbers. 
              Therefore, the first step in any NLP model is <strong>Tokenization</strong>.
            </p>
            <p className="mb-4">
              We break down a sentence into smaller chunks called "Tokens". These can be words, characters, or sub-words.
            </p>
            <TokenizationDemo lang="en" />
            <p className="mb-4">
              Once tokenized, each token is assigned a unique integer ID from a vocabulary.
            </p>
          </div>
        )
      },
      {
        id: '0-2',
        title: 'Input Embeddings',
        content: (
          <div>
            <p className="mb-4">
              Integer IDs are not enough (e.g., ID 100 is not "twice" ID 50). We convert these integers into dense vectors of size <MathBlock formula="d_{model}" /> (usually 512).
            </p>
            <p className="mb-4">
              These embeddings are learned parameters. During training, the model learns that words with similar meanings (like "King" and "Queen") should have similar vector representations in this 512-dimensional space.
            </p>
            <MathBlock formula="X_{\text{embedding}} = \text{EmbeddingLookup}(x_{\text{input}}) \times \sqrt{d_{model}}" block />
            <p className="text-sm text-slate-500">Note: In the Transformer paper, embeddings are multiplied by <MathBlock formula="\sqrt{d_{model}}" /> to stabilize variance before adding positional encoding.</p>
            
            <QuizSection 
              lang="en"
              question={{
                id: 'q0',
                question: 'Why do we use Embeddings instead of One-Hot Encoding?',
                options: [
                  'Embeddings are easier to calculate.',
                  'Embeddings capture semantic relationships and are less sparse.',
                  'One-Hot vectors are too small.',
                  'Embeddings remove the need for tokenization.'
                ],
                correctAnswer: 1,
                explanation: 'One-hot vectors are extremely sparse and high-dimensional, and they treat every word as equidistant. Dense embeddings capture semantic similarity (e.g., dog is close to cat).'
              }} 
            />
          </div>
        )
      }
    ]
  },
  {
    id: 'ch1',
    title: '1. Background & Motivation',
    description: 'Why do we need the Transformer?',
    sections: [
      {
        id: '1-1',
        title: 'Pain Points of Recurrent Models',
        content: (
          <div>
            <p className="mb-4">
              Before 2017, sequence modeling (like translation) was dominated by RNNs and LSTMs.
              These models process data <strong>sequentially</strong>.
            </p>
            <ul className="list-disc ml-6 space-y-2 mb-4">
              <li><strong>Sequential Computation:</strong> To compute hidden state <MathBlock formula="h_t" />, you need <MathBlock formula="h_{t-1}" />. This precludes parallelization.</li>
              <li><strong>Long-Term Dependencies:</strong> Information from the beginning of a long sentence often fades before reaching the end.</li>
            </ul>
            <div className="bg-red-50 border-l-4 border-red-500 p-4">
              <strong>The Goal:</strong> Create a model that is highly parallelizable and can relate any two positions in a sequence instantly.
            </div>
            
            <h4 className="font-bold mt-6 mb-2">Complexity Comparison</h4>
            <div className="overflow-x-auto">
              <table className="min-w-full text-sm text-left">
                <thead className="bg-slate-100 font-bold">
                  <tr>
                    <th className="p-2">Layer Type</th>
                    <th className="p-2">Complexity per Layer</th>
                    <th className="p-2">Sequential Ops</th>
                    <th className="p-2">Max Path Length</th>
                  </tr>
                </thead>
                <tbody>
                  <tr className="border-b">
                    <td className="p-2">RNN</td>
                    <td className="p-2"><MathBlock formula="O(n \cdot d^2)" /></td>
                    <td className="p-2"><MathBlock formula="O(n)" /></td>
                    <td className="p-2"><MathBlock formula="O(n)" /></td>
                  </tr>
                  <tr>
                    <td className="p-2 bg-brand-50 font-bold text-brand-700">Self-Attention</td>
                    <td className="p-2 bg-brand-50"><MathBlock formula="O(n^2 \cdot d)" /></td>
                    <td className="p-2 bg-brand-50"><MathBlock formula="O(1)" /></td>
                    <td className="p-2 bg-brand-50"><MathBlock formula="O(1)" /></td>
                  </tr>
                </tbody>
              </table>
            </div>
            <p className="text-xs text-slate-500 mt-2">
              <MathBlock formula="n" /> is sequence length, <MathBlock formula="d" /> is representation dimension.
              Self-Attention is faster for shorter sequences where <MathBlock formula="n < d" />.
            </p>
          </div>
        )
      },
      {
        id: '1-2',
        title: 'Attention is All You Need',
        content: (
          <div>
            <p className="mb-4">
              The paper proposes that we don't need recurrence (RNNs) or convolution (CNNs). Instead, we can rely entirely on an <strong>Attention Mechanism</strong> to draw global dependencies between input and output.
            </p>
            <QuizSection 
              lang="en"
              question={{
                id: 'q1',
                question: 'What is the primary advantage of Transformer over RNNs regarding training?',
                options: [
                  'It has fewer parameters.',
                  'It allows significant parallelization (O(1) sequential ops).',
                  'It uses Convolutional Neural Networks.',
                  'It requires no data preprocessing.'
                ],
                correctAnswer: 1,
                explanation: 'Because the Transformer processes the entire sequence at once using Attention (instead of step-by-step), it can fully utilize modern GPU parallelism.'
              }} 
            />
          </div>
        )
      }
    ]
  },
  {
    id: 'ch2',
    title: '2. Architecture Overview',
    description: 'The High-Level Encoder-Decoder Structure',
    sections: [
      {
        id: '2-1',
        title: 'The Big Picture',
        content: (
          <div>
            <p className="mb-4">
              The Transformer follows an Encoder-Decoder architecture.
            </p>
            <ArchitectureDiagram lang="en" />
            <div className="grid md:grid-cols-2 gap-4">
              <div className="bg-white p-4 rounded shadow-sm border">
                <h4 className="font-bold text-brand-600 mb-2">Encoder (Left)</h4>
                <p className="text-sm">Takes the input sequence (e.g., English sentence) and maps it to a continuous representation holding the meaning. It consists of a stack of $N=6$ identical layers.</p>
              </div>
              <div className="bg-white p-4 rounded shadow-sm border">
                <h4 className="font-bold text-pink-600 mb-2">Decoder (Right)</h4>
                <p className="text-sm">Takes the Encoder's output and generates the target sequence (e.g., French translation) one element at a time. It also has $N=6$ layers.</p>
              </div>
            </div>
            
            <QuizSection 
              lang="en"
              question={{
                id: 'q2',
                question: 'What information does the Decoder receive?',
                options: [
                  'Only the target sentence.',
                  'Only the source sentence.',
                  'The output of the Encoder AND the target generated so far.',
                  'Random noise.'
                ],
                correctAnswer: 2,
                explanation: 'The Decoder has two sources of info: Self-Attention (looking at what it has generated so far) and Cross-Attention (looking at the Encoder output).'
              }} 
            />
          </div>
        )
      }
    ]
  },
  {
    id: 'ch3',
    title: '3. Core Components (Deep Dive)',
    description: 'Understanding the mechanics',
    sections: [
      {
        id: '3-1',
        title: 'Self-Attention',
        difficulty: 'advanced',
        content: (
          <div>
            <p className="mb-4">
              The heart of the Transformer. It allows the model to look at other words in the input sentence to better understand the current word.
            </p>
            <div className="bg-blue-50 p-4 rounded-lg mb-4 border border-blue-100">
                <h4 className="font-bold text-blue-800 mb-2">Example:</h4>
                <p className="italic text-slate-700">"The animal didn't cross the street because <strong>it</strong> was too tired."</p>
                <p className="mt-2 text-sm">
                    When the model processes the word "it", Self-Attention allows it to associate "it" strongly with "animal". 
                    Without this, the model wouldn't know if "it" referred to the street or the animal.
                </p>
            </div>
            <p className="mb-4">
              For every input token, we create three vectors: 
              <strong>Query ($Q$)</strong>, <strong>Key ($K$)</strong>, and <strong>Value ($V$)</strong>.
            </p>
            <MathBlock formula="\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V" block />
            
            <CodeBlock code={`# 1. Scaled Dot-Product Attention
def attention(query, key, value):
    d_k = query.size(-1)
    
    # Matmul Q and K -> Scores
    scores = torch.matmul(query, key.transpose(-2, -1))
    
    # Scale by sqrt(d_k)
    scores = scores / math.sqrt(d_k)
    
    # Softmax to get probabilities
    attn_weights = F.softmax(scores, dim=-1)
    
    # Multiply by V
    return torch.matmul(attn_weights, value)`} />

            <InteractiveAttention lang="en" />
            
             <QuizSection 
              lang="en"
              question={{
                id: 'q3-1',
                question: 'In the equation, why do we divide by sqrt(d_k)?',
                options: [
                  'To reduce computation time.',
                  'To prevent the dot products from growing too large, which would push Softmax into regions with small gradients.',
                  'To make the matrix multiplication valid.',
                  'It is an arbitrary constant.'
                ],
                correctAnswer: 1,
                explanation: 'Large dot products result in Softmax outputs close to 0 or 1, where gradients are extremely small (vanishing gradients). Scaling prevents this.'
              }} 
            />
          </div>
        )
      },
      {
        id: '3-2',
        title: 'Multi-Head Attention',
        content: (
          <div>
            <p className="mb-4">
              Instead of performing a single attention function, we do it $h$ times in parallel with different linear projections. This allows the model to attend to information from different representation subspaces.
            </p>
            <div className="p-4 bg-brand-50 rounded-lg text-sm text-brand-900 mb-4">
              <strong>Analogy:</strong> Imagine reading a book with 8 different colored highlighters. 
              The "Yellow" head focuses on dates, the "Blue" head focuses on names, and the "Green" head focuses on actions.
              Combining them gives you a complete understanding.
            </div>
            <CodeBlock code={`class MultiHeadAttention(nn.Module):
    def forward(self, x):
        batch_size = x.size(0)
        
        # 1. Linear projections for Q, K, V
        # Split into 'h' heads
        Q = self.w_q(x).view(batch_size, -1, self.heads, self.d_k)
        K = self.w_k(x).view(batch_size, -1, self.heads, self.d_k)
        V = self.w_v(x).view(batch_size, -1, self.heads, self.d_k)
        
        # 2. Apply attention to each head
        # (Simplified, actually done via matrix ops)
        out = attention(Q, K, V)
        
        # 3. Concatenate and Linear
        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        return self.w_o(out)`} />
          </div>
        )
      },
      {
        id: '3-3',
        title: 'Positional Encoding',
        difficulty: 'intermediate',
        content: (
          <div>
            <p className="mb-4">
              Since the Transformer has no recurrence, it has no notion of "order". We must inject position info directly into the embeddings.
            </p>
            <div className="bg-yellow-50 p-4 rounded-lg mb-4 border border-yellow-100">
                <h4 className="font-bold text-yellow-800 mb-2">Analogy:</h4>
                <p className="text-sm">
                    Imagine a library where books are thrown in a pile (Bag of Words). You don't know the story order.
                    Positional Encoding is like stamping a page number on each word so the model knows where it belongs, 
                    even if processed simultaneously.
                </p>
            </div>
            <MathBlock formula="PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{\\text{model}}})" block />
            <MathBlock formula="PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{\\text{model}}})" block />
            
            <PositionalEncodingViz lang="en" />
          </div>
        )
      },
      {
        id: '3-4',
        title: 'Feed-Forward Networks (FFN)',
        content: (
          <div>
             <p className="mb-4">
              In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.
            </p>
            <p className="mb-4">
              It consists of two linear transformations with a ReLU activation in between.
            </p>
            <MathBlock formula="\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2" block />
            <p className="text-sm text-slate-600 mb-4">
                The dimensionality of input and output is <MathBlock formula="d_{model} = 512" />, and the inner-layer has dimensionality <MathBlock formula="d_{ff} = 2048" />.
            </p>
            <CodeBlock code={`class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        super(PositionwiseFeedForward, self).__init__()
        # Expands dimension (512 -> 2048)
        self.w_1 = nn.Linear(d_model, d_ff) 
        # Restores dimension (2048 -> 512)
        self.w_2 = nn.Linear(d_ff, d_model)

    def forward(self, x):
        # ReLU activation in between
        return self.w_2(F.relu(self.w_1(x)))`} />
          </div>
        )
      },
      {
        id: '3-5',
        title: 'Encoder-Decoder Attention',
        difficulty: 'advanced',
        content: (
           <div>
            <p className="mb-4">
              This is the specific layer in the <strong>Decoder</strong> that allows it to look at the <strong>Encoder's</strong> output.
            </p>
            <ul className="list-disc ml-6 space-y-2 mb-4">
                <li><strong>Queries (Q):</strong> Come from the previous decoder layer (what we are currently translating).</li>
                <li><strong>Keys (K) & Values (V):</strong> Come from the Encoder output (the source sentence).</li>
            </ul>
            <div className="bg-purple-50 p-4 rounded-lg mb-4 border border-purple-100">
                <h4 className="font-bold text-purple-800 mb-2">Why?</h4>
                <p className="text-sm">
                   This aligns the translation with the original text. If the Decoder is trying to generate the French word for "student", 
                   this mechanism allows it to focus on the English word "student" encoded by the Encoder.
                </p>
            </div>
           </div>
        )
      },
      {
        id: '3-6',
        title: 'Add & Norm',
        content: (
            <div>
                <p className="mb-4">
                    The output of each sub-layer (Self-Attention, FFN) is calculated as:
                </p>
                <MathBlock formula="\\text{LayerNorm}(x + \\text{Sublayer}(x))" block />
                <p className="mb-4">
                    <strong>Residual Connection (Add):</strong> We add the input $x$ back to the output. This solves the "vanishing gradient" problem in deep networks.
                    <br />
                    <strong>Layer Normalization (Norm):</strong> We normalize the statistics of the hidden vector to stabilize training.
                </p>
            </div>
        )
      }
    ]
  },
  {
    id: 'ch4',
    title: '4. Training & Details',
    description: 'How to make it learn',
    sections: [
      {
        id: '4-1',
        title: 'Masking',
        content: (
          <div>
            <h3 className="font-bold text-lg mb-2 text-slate-800">Technical Explanation</h3>
            <p className="mb-4">
              <strong>Padding Mask:</strong> Used to ignore "pad" tokens (usually index 0) in the input batch so they don't affect gradients.
              <br/>
              <strong>Look-Ahead Mask:</strong> Crucial for the Decoder. When predicting the token at position $t$, the model must not attend to tokens at $t+1$. We set their attention scores to $-\\infty$ before Softmax, resulting in 0 probability.
            </p>
            
            <div className="bg-green-50 p-4 rounded-lg border-l-4 border-green-500 mt-4">
                <h4 className="font-bold mb-1 text-green-800">Layman's Understanding</h4>
                <p className="text-sm text-green-900">
                    Imagine taking a fill-in-the-blank test. You want to learn to predict the next word.
                    If you can see the future words (the answers) while guessing the current one, you aren't learning.
                    <strong>Look-Ahead Masking</strong> is like covering the rest of the sentence with a piece of paper so you can only see what you've written so far.
                </p>
            </div>
            
             <QuizSection 
              lang="en"
              question={{
                id: 'q4-1',
                question: 'Why do we need a Look-Ahead Mask in the Decoder but not the Encoder?',
                options: [
                  'Because the Encoder is bidirectional (sees whole sentence), while Decoder generates sequentially.',
                  'The Encoder does not use Self-Attention.',
                  'The Decoder is faster.',
                  'Padding tokens only exist in the Decoder.'
                ],
                correctAnswer: 0,
                explanation: 'The Encoder processes the full source sentence at once. The Decoder is autoregressive, meaning it generates one word at a time and shouldn\'t "cheat" by seeing future words.'
              }} 
            />
          </div>
        )
      },
      {
        id: '4-2',
        title: 'Optimizer & Regularization',
        content: (
          <div>
            <h3 className="font-bold text-lg mb-2 text-slate-800">Optimizer</h3>
            <p className="mb-4">
              The paper uses the <strong>Adam</strong> optimizer with specific $\beta_1=0.9, \beta_2=0.98$.
              Crucially, it uses a <strong>Learning Rate Schedule</strong> with a "warmup" phase.
            </p>
            <MathBlock formula="lrate = d_{\\text{model}}^{-0.5} \\cdot \\min(step\\_num^{-0.5}, step\\_num \\cdot warmup\\_steps^{-1.5})" block />
            
            <h3 className="font-bold text-lg mb-2 mt-6 text-slate-800">Label Smoothing</h3>
            <p className="mb-4">
              Instead of forcing the model to be 100% confident (Target: 1.0 for correct word, 0 for others), we smooth the target distribution.
              <br/>
              If smoothing <MathBlock formula="\epsilon_{ls} = 0.1" />, the correct word gets probability 0.9, and the rest of the probability mass is distributed among other words.
            </p>
            <p className="text-sm text-slate-600 mb-4">
                This hurts perplexity (uncertainty) but improves accuracy and BLEU score by preventing the model from becoming over-confident and overfitting.
            </p>
          </div>
        )
      }
    ]
  },
  {
      id: 'ch5',
      title: '5. Inference & Decoding',
      description: 'Generating text',
      sections: [
          {
              id: '5-1',
              title: 'Auto-Regressive Generation',
              content: (
                  <div>
                      <p className="mb-4">
                          During inference (translation), the model generates words one by one.
                      </p>
                      <ol className="list-decimal ml-6 space-y-2">
                          <li>Pass the source sentence to Encoder.</li>
                          <li>Give Decoder a special <code>&lt;START&gt;</code> token.</li>
                          <li>Decoder outputs probability distribution for the first word.</li>
                          <li>Pick the best word, add it to the input.</li>
                          <li>Repeat until <code>&lt;END&gt;</code> token is produced.</li>
                      </ol>
                  </div>
              )
          },
          {
              id: '5-2',
              title: 'Greedy vs Beam Search',
              content: (
                  <div>
                      <p className="mb-4">
                          <strong>Greedy Search:</strong> Always pick the word with the highest probability at each step. Fast, but can lead to suboptimal sentences (local optimum).
                      </p>
                      <p className="mb-4">
                          <strong>Beam Search:</strong> Keep track of the top $k$ (beam width) most likely sentences at each step. This explores multiple possibilities simultaneously to find a better overall translation.
                      </p>
                       <QuizSection 
                        lang="en"
                        question={{
                          id: 'q5',
                          question: 'What is the main benefit of Beam Search over Greedy Search?',
                          options: [
                            'It is faster.',
                            'It explores multiple potential sentence paths to avoid getting stuck in local optima.',
                            'It uses less memory.',
                            'It doesn\'t require a Decoder.'
                          ],
                          correctAnswer: 1,
                          explanation: 'Greedy search might pick a word that looks good now but leads to a dead end. Beam search keeps options open longer.'
                        }} 
                      />
                  </div>
              )
          }
      ]
  },
  {
      id: 'ch6',
      title: '6. The Transformer Family',
      description: 'Evolution of the architecture',
      sections: [
          {
              id: '6-1',
              title: 'Encoder-Only (BERT)',
              content: (
                  <div>
                      <h4 className="font-bold text-slate-800">BERT (Bidirectional Encoder Representations from Transformers)</h4>
                      <p className="text-sm mb-2">
                          Uses only the <strong>Encoder</strong> stack.
                      </p>
                      <p className="text-sm mb-2">
                          <strong>Goal:</strong> Understanding text. It looks at the whole sentence at once (Bidirectional).
                      </p>
                      <p className="text-sm">
                          <strong>Tasks:</strong> Classification, Sentiment Analysis, Named Entity Recognition.
                      </p>
                  </div>
              )
          },
          {
              id: '6-2',
              title: 'Decoder-Only (GPT)',
              content: (
                  <div>
                      <h4 className="font-bold text-slate-800">GPT (Generative Pre-trained Transformer)</h4>
                      <p className="text-sm mb-2">
                          Uses only the <strong>Decoder</strong> stack (with masked self-attention).
                      </p>
                      <p className="text-sm mb-2">
                          <strong>Goal:</strong> Generating text. It predicts the next word based on previous words.
                      </p>
                      <p className="text-sm">
                          <strong>Tasks:</strong> Text generation, Chatbots, Code completion.
                      </p>
                  </div>
              )
          },
          {
              id: '6-3',
              title: 'Encoder-Decoder (T5 / BART)',
              content: (
                  <div>
                      <h4 className="font-bold text-slate-800">T5 (Text-to-Text Transfer Transformer)</h4>
                      <p className="text-sm mb-2">
                          Uses the full original architecture.
                      </p>
                      <p className="text-sm">
                          <strong>Tasks:</strong> Translation, Summarization (Sequence-to-Sequence tasks).
                      </p>
                  </div>
              )
          }
      ]
  },
  {
    id: 'ch7',
    title: '7. Experimental Results',
    description: 'Does it actually work?',
    sections: [
      {
        id: '7-1',
        title: 'Performance & Efficiency',
        content: (
          <div>
            <p className="mb-4">
                The Transformer achieved state-of-the-art results on English-to-German and English-to-French translation tasks (WMT 2014), while requiring significantly less training time than RNN/CNN based predecessors.
            </p>
            <div className="h-64 w-full mb-6">
              <ResponsiveContainer width="100%" height="100%">
                <BarChart data={perfData} layout="vertical" margin={{ top: 5, right: 30, left: 40, bottom: 5 }}>
                  <CartesianGrid strokeDasharray="3 3" />
                  <XAxis type="number" domain={[20, 30]} />
                  <YAxis dataKey="name" type="category" width={100} style={{fontSize: '12px'}} />
                  <Tooltip />
                  <Legend />
                  <Bar dataKey="bleu" name="BLEU Score" fill="#0ea5e9" />
                </BarChart>
              </ResponsiveContainer>
              <p className="text-center text-xs text-slate-500 mt-2">Comparison of BLEU scores (Higher is better)</p>
            </div>
            
            <div className="grid grid-cols-2 gap-4">
                <div className="p-4 border rounded bg-white">
                    <h4 className="font-bold text-green-600">Training Cost</h4>
                    <p className="text-sm mt-1">
                        The Transformer (Base) cost only <strong>$3.3 \cdot 10^{18}$</strong> floating point operations to train.
                        The Big model took just <strong>3.5 days</strong> on 8 P100 GPUs, whereas previous SOTA models took weeks.
                    </p>
                </div>
                <div className="p-4 border rounded bg-white">
                    <h4 className="font-bold text-purple-600">Generalization</h4>
                    <p className="text-sm mt-1">
                        The paper demonstrated the model generalizes well to other tasks. For example, it performed exceptionally well on <strong>English Constituency Parsing</strong> with minimal task-specific tuning, proving it's not just for translation.
                    </p>
                </div>
            </div>
          </div>
        )
      }
    ]
  },
  {
    id: 'ch8',
    title: '8. Resources',
    description: 'Further exploration',
    sections: [
      {
        id: '8-1',
        title: 'Official Implementations',
        content: (
          <div>
            <p className="mb-4">
                The authors released the original code in the Tensor2Tensor library. Below are direct links to the source material.
            </p>
            <ul className="space-y-4">
                <li>
                    <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noreferrer" className="flex items-center gap-2 p-4 border rounded hover:bg-slate-50 transition group">
                        <span className="text-2xl">ğŸ“„</span>
                        <div>
                            <div className="font-bold text-brand-600 group-hover:underline">Original Paper (ArXiv)</div>
                            <div className="text-sm text-slate-500">Attention Is All You Need (Vaswani et al., 2017)</div>
                        </div>
                    </a>
                </li>
                <li>
                    <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noreferrer" className="flex items-center gap-2 p-4 border rounded hover:bg-slate-50 transition group">
                        <span className="text-2xl">ğŸ’»</span>
                        <div>
                            <div className="font-bold text-slate-800 group-hover:underline">Tensor2Tensor (GitHub)</div>
                            <div className="text-sm text-slate-500">The official TensorFlow implementation used in the paper.</div>
                        </div>
                    </a>
                </li>
                 <li>
                    <a href="https://github.com/pytorch/fairseq" target="_blank" rel="noreferrer" className="flex items-center gap-2 p-4 border rounded hover:bg-slate-50 transition group">
                        <span className="text-2xl">ğŸ”¥</span>
                        <div>
                            <div className="font-bold text-slate-800 group-hover:underline">PyTorch FairSeq (GitHub)</div>
                            <div className="text-sm text-slate-500">Facebook AI Research's toolkit, containing high-quality Transformer implementations.</div>
                        </div>
                    </a>
                </li>
            </ul>
          </div>
        )
      }
    ]
  }
];

const chaptersZh: Chapter[] = [
  {
    id: 'ch0',
    title: '0. è¾“å…¥ä¸åµŒå…¥',
    description: 'åœ¨è¿›å…¥æ¶æ„ä¹‹å‰ï¼šæœºå™¨å¦‚ä½•é˜…è¯»æ–‡æœ¬',
    sections: [
      {
        id: '0-1',
        title: 'ä»æ–‡æœ¬åˆ°æ•°å­—',
        difficulty: 'basic',
        content: (
          <div>
            <p className="mb-4">
              è®¡ç®—æœºæ— æ³•ç†è§£åƒ "Hello" è¿™æ ·çš„åŸå§‹æ–‡æœ¬ï¼Œå®ƒä»¬åªèƒ½å¤„ç†æ•°å­—ã€‚
              å› æ­¤ï¼Œä»»ä½• NLP æ¨¡å‹çš„ç¬¬ä¸€æ­¥éƒ½æ˜¯ <strong>åˆ†è¯ (Tokenization)</strong>ã€‚
            </p>
            <p className="mb-4">
              æˆ‘ä»¬å°†å¥å­åˆ†è§£æˆæ›´å°çš„å—ï¼Œç§°ä¸º "Token"ã€‚è¿™äº›å¯ä»¥æ˜¯å•è¯ã€å­—ç¬¦æˆ–å­è¯ã€‚
            </p>
            <TokenizationDemo lang="zh" />
            <p className="mb-4">
              åˆ†è¯åï¼Œæ¯ä¸ª Token éƒ½ä¼šä»è¯æ±‡è¡¨ä¸­è¢«åˆ†é…ä¸€ä¸ªå”¯ä¸€çš„æ•´æ•° IDã€‚
            </p>
          </div>
        )
      },
      {
        id: '0-2',
        title: 'è¾“å…¥åµŒå…¥ (Input Embeddings)',
        content: (
          <div>
            <p className="mb-4">
              æ•´æ•° ID æ˜¯ä¸å¤Ÿçš„ï¼ˆä¾‹å¦‚ï¼ŒID 100 å¹¶ä¸æ„å‘³ç€å®ƒæ˜¯ ID 50 çš„ä¸¤å€ï¼‰ã€‚æˆ‘ä»¬å°†è¿™äº›æ•´æ•°è½¬æ¢ä¸ºå¤§å°ä¸º <MathBlock formula="d_{model}" />ï¼ˆé€šå¸¸ä¸º 512ï¼‰çš„å¯†é›†å‘é‡ã€‚
            </p>
            <p className="mb-4">
              è¿™äº›åµŒå…¥æ˜¯å¯å­¦ä¹ çš„å‚æ•°ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šå­¦ä¹ åˆ°å…·æœ‰ç›¸ä¼¼å«ä¹‰çš„å•è¯ï¼ˆå¦‚â€œå›½ç‹â€å’Œâ€œç‹åâ€ï¼‰åœ¨è¿™ä¸ª 512 ç»´ç©ºé—´ä¸­åº”è¯¥å…·æœ‰ç›¸ä¼¼çš„å‘é‡è¡¨ç¤ºã€‚
            </p>
            <MathBlock formula="X_{\text{embedding}} = \text{EmbeddingLookup}(x_{\text{input}}) \times \sqrt{d_{model}}" block />
            <p className="text-sm text-slate-500">æ³¨æ„ï¼šåœ¨ Transformer è®ºæ–‡ä¸­ï¼ŒåµŒå…¥åœ¨åŠ ä¸Šä½ç½®ç¼–ç ä¹‹å‰ä¼šä¹˜ä»¥ <MathBlock formula="\sqrt{d_{model}}" /> ä»¥ç¨³å®šæ–¹å·®ã€‚</p>
            
            <QuizSection 
              lang="zh"
              question={{
                id: 'q0',
                question: 'ä¸ºä»€ä¹ˆæˆ‘ä»¬ä½¿ç”¨ Embedding è€Œä¸æ˜¯ One-Hot ç¼–ç ï¼Ÿ',
                options: [
                  'Embedding æ›´å®¹æ˜“è®¡ç®—ã€‚',
                  'Embedding èƒ½æ•æ‰è¯­ä¹‰å…³ç³»ï¼Œä¸”æ›´åŠ ç´§å‡‘ï¼ˆéç¨€ç–ï¼‰ã€‚',
                  'One-Hot å‘é‡å¤ªå°äº†ã€‚',
                  'Embedding è®©æˆ‘ä»¬ä¸éœ€è¦åˆ†è¯ã€‚'
                ],
                correctAnswer: 1,
                explanation: 'One-hot å‘é‡éå¸¸ç¨€ç–ä¸”ç»´åº¦æé«˜ï¼Œå¹¶ä¸”å®ƒè®¤ä¸ºæ‰€æœ‰è¯ä¹‹é—´çš„è·ç¦»éƒ½æ˜¯ç›¸ç­‰çš„ã€‚å¯†é›† Embedding å¯ä»¥æ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§ï¼ˆä¾‹å¦‚ï¼ŒçŒ«å’Œç‹—çš„å‘é‡è·ç¦»å¾ˆè¿‘ï¼‰ã€‚'
              }} 
            />
          </div>
        )
      }
    ]
  },
  {
    id: 'ch1',
    title: '1. èƒŒæ™¯ä¸åŠ¨æœº',
    description: 'ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦ Transformerï¼Ÿ',
    sections: [
      {
        id: '1-1',
        title: 'ä¼ ç»Ÿæ¨¡å‹çš„ç—›ç‚¹',
        content: (
          <div>
            <p className="mb-4">
              åœ¨2017å¹´ä¹‹å‰ï¼Œåºåˆ—å»ºæ¨¡ï¼ˆå¦‚æœºå™¨ç¿»è¯‘ï¼‰ä¸»è¦ç”± RNN å’Œ LSTM ä¸»å¯¼ã€‚
              è¿™äº›æ¨¡å‹ä»¥<strong>ä¸²è¡Œæ–¹å¼</strong>å¤„ç†æ•°æ®ã€‚
            </p>
            <ul className="list-disc ml-6 space-y-2 mb-4">
              <li><strong>ä¸²è¡Œè®¡ç®—ï¼š</strong> è®¡ç®—éšè—çŠ¶æ€ <MathBlock formula="h_t" /> å¿…é¡»ä¾èµ– <MathBlock formula="h_{t-1}" />ã€‚è¿™ä½¿å¾—æ— æ³•è¿›è¡Œå¹¶è¡Œè®¡ç®—ã€‚</li>
              <li><strong>é•¿è·ç¦»ä¾èµ–ï¼š</strong> åœ¨å¤„ç†é•¿å¥å­æ—¶ï¼Œå¼€å¤´çš„ä¿¡æ¯å¾€å¾€åœ¨åˆ°è¾¾å¥å­æœ«å°¾æ—¶å·²ç»ä¸¢å¤±ã€‚</li>
            </ul>
            <div className="bg-red-50 border-l-4 border-red-500 p-4">
              <strong>ç›®æ ‡ï¼š</strong> åˆ›å»ºä¸€ä¸ªé«˜åº¦å¹¶è¡ŒåŒ–çš„æ¨¡å‹ï¼Œå¹¶èƒ½ç¬é—´å»ºç«‹åºåˆ—ä¸­ä»»æ„ä¸¤ä¸ªä½ç½®çš„è”ç³»ã€‚
            </div>
             <h4 className="font-bold mt-6 mb-2">å¤æ‚åº¦å¯¹æ¯”</h4>
            <div className="overflow-x-auto">
              <table className="min-w-full text-sm text-left">
                <thead className="bg-slate-100 font-bold">
                  <tr>
                    <th className="p-2">å±‚ç±»å‹</th>
                    <th className="p-2">æ¯å±‚å¤æ‚åº¦</th>
                    <th className="p-2">ä¸²è¡Œæ“ä½œæ•°</th>
                    <th className="p-2">æœ€å¤§è·¯å¾„é•¿åº¦</th>
                  </tr>
                </thead>
                <tbody>
                  <tr className="border-b">
                    <td className="p-2">RNN</td>
                    <td className="p-2"><MathBlock formula="O(n \cdot d^2)" /></td>
                    <td className="p-2"><MathBlock formula="O(n)" /></td>
                    <td className="p-2"><MathBlock formula="O(n)" /></td>
                  </tr>
                  <tr>
                    <td className="p-2 bg-brand-50 font-bold text-brand-700">Self-Attention</td>
                    <td className="p-2 bg-brand-50"><MathBlock formula="O(n^2 \cdot d)" /></td>
                    <td className="p-2 bg-brand-50"><MathBlock formula="O(1)" /></td>
                    <td className="p-2 bg-brand-50"><MathBlock formula="O(1)" /></td>
                  </tr>
                </tbody>
              </table>
            </div>
          </div>
        )
      },
      {
        id: '1-2',
        title: 'Attention Is All You Need',
        content: (
          <div>
            <p className="mb-4">
              è®ºæ–‡æå‡ºæˆ‘ä»¬ä¸å†éœ€è¦å¾ªç¯ï¼ˆRNNï¼‰æˆ–å·ç§¯ï¼ˆCNNï¼‰ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥å®Œå…¨ä¾èµ–<strong>æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism)</strong> æ¥æ•æ‰è¾“å…¥ä¸è¾“å‡ºä¹‹é—´çš„å…¨å±€ä¾èµ–å…³ç³»ã€‚
            </p>
             <QuizSection 
              lang="zh"
              question={{
                id: 'q1',
                question: 'Transformer ç›¸æ¯” RNN åœ¨è®­ç»ƒä¸Šçš„ä¸»è¦ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ',
                options: [
                  'å®ƒçš„å‚æ•°æ›´å°‘ã€‚',
                  'å®ƒå…è®¸å¤§è§„æ¨¡å¹¶è¡ŒåŒ–ï¼ˆO(1) ä¸²è¡Œæ“ä½œï¼‰ã€‚',
                  'å®ƒä½¿ç”¨äº†å·ç§¯ç¥ç»ç½‘ç»œã€‚',
                  'å®ƒä¸éœ€è¦æ•°æ®é¢„å¤„ç†ã€‚'
                ],
                correctAnswer: 1,
                explanation: 'å› ä¸º Transformer ä½¿ç”¨ Attention ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªåºåˆ—ï¼ˆè€Œä¸æ˜¯ä¸€æ­¥æ¥ä¸€æ­¥ï¼‰ï¼Œå®ƒå¯ä»¥å……åˆ†åˆ©ç”¨ç°ä»£ GPU çš„å¹¶è¡Œè®¡ç®—èƒ½åŠ›ã€‚'
              }} 
            />
          </div>
        )
      }
    ]
  },
  {
    id: 'ch2',
    title: '2. æ•´ä½“æ¶æ„æ¦‚è§ˆ',
    description: 'å®è§‚è§†è§’çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„',
    sections: [
      {
        id: '2-1',
        title: 'å…¨æ™¯å›¾',
        content: (
          <div>
            <p className="mb-4">
              Transformer éµå¾ªç¼–ç å™¨-è§£ç å™¨ (Encoder-Decoder) æ¶æ„ã€‚
            </p>
            <ArchitectureDiagram lang="zh" />
            <div className="grid md:grid-cols-2 gap-4">
              <div className="bg-white p-4 rounded shadow-sm border">
                <h4 className="font-bold text-brand-600 mb-2">Encoder (ç¼–ç å™¨ - å·¦ä¾§)</h4>
                <p className="text-sm">æ¥æ”¶è¾“å…¥åºåˆ—ï¼ˆä¾‹å¦‚ï¼šè‹±æ–‡å¥å­ï¼‰å¹¶å°†å…¶æ˜ å°„ä¸ºåŒ…å«è¯­ä¹‰ä¿¡æ¯çš„è¿ç»­è¡¨ç¤ºã€‚å®ƒç”± $N=6$ å±‚ç›¸åŒçš„ç½‘ç»œå †å è€Œæˆã€‚</p>
              </div>
              <div className="bg-white p-4 rounded shadow-sm border">
                <h4 className="font-bold text-pink-600 mb-2">Decoder (è§£ç å™¨ - å³ä¾§)</h4>
                <p className="text-sm">æ¥æ”¶ç¼–ç å™¨çš„è¾“å‡ºï¼Œå¹¶é€ä¸ªå…ƒç´ ç”Ÿæˆç›®æ ‡åºåˆ—ï¼ˆä¾‹å¦‚ï¼šæ³•æ–‡ç¿»è¯‘ï¼‰ã€‚å®ƒåŒæ ·ç”± $N=6$ å±‚å †å è€Œæˆã€‚</p>
              </div>
            </div>
             <QuizSection 
              lang="zh"
              question={{
                id: 'q2',
                question: 'è§£ç å™¨ (Decoder) æ¥æ”¶ä»€ä¹ˆä¿¡æ¯ï¼Ÿ',
                options: [
                  'åªæœ‰ç›®æ ‡å¥å­ã€‚',
                  'åªæœ‰æºå¥å­ã€‚',
                  'ç¼–ç å™¨çš„è¾“å‡º ä»¥åŠ ç›®å‰ä¸ºæ­¢ç”Ÿæˆçš„ç›®æ ‡åºåˆ—ã€‚',
                  'éšæœºå™ªå£°ã€‚'
                ],
                correctAnswer: 2,
                explanation: 'è§£ç å™¨æœ‰ä¸¤ä¸ªä¿¡æ¯æ¥æºï¼šè‡ªæ³¨æ„åŠ›ï¼ˆæŸ¥çœ‹è‡ªå·±ç›®å‰ç”Ÿæˆäº†ä»€ä¹ˆï¼‰å’Œ äº¤å‰æ³¨æ„åŠ›ï¼ˆæŸ¥çœ‹ç¼–ç å™¨çš„è¾“å‡ºï¼‰ã€‚'
              }} 
            />
          </div>
        )
      }
    ]
  },
  {
    id: 'ch3',
    title: '3. æ ¸å¿ƒç»„ä»¶è¯¦è§£',
    description: 'æ·±å…¥å‰–æå†…éƒ¨æœºåˆ¶',
    sections: [
      {
        id: '3-1',
        title: 'è‡ªæ³¨æ„åŠ›æœºåˆ¶ (Self-Attention)',
        difficulty: 'advanced',
        content: (
          <div>
            <p className="mb-4">
              è¿™æ˜¯ Transformer çš„æ ¸å¿ƒã€‚å®ƒå…è®¸æ¨¡å‹åœ¨å¤„ç†å½“å‰è¯æ—¶ï¼Œå…³æ³¨è¾“å…¥å¥å­ä¸­çš„å…¶ä»–è¯ï¼Œä»è€Œæ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡ã€‚
            </p>
            <div className="bg-blue-50 p-4 rounded-lg mb-4 border border-blue-100">
                <h4 className="font-bold text-blue-800 mb-2">ä¸¾ä¸ªæ —å­ï¼š</h4>
                <p className="italic text-slate-700">"The animal didn't cross the street because <strong>it</strong> was too tired."</p>
                <p className="mt-2 text-sm">
                    å½“æ¨¡å‹å¤„ç† "it"ï¼ˆå®ƒï¼‰è¿™ä¸ªè¯æ—¶ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶ä¼šå°†å®ƒä¸ "animal"ï¼ˆåŠ¨ç‰©ï¼‰å¼ºçƒˆå…³è”èµ·æ¥ã€‚
                    å¦‚æœæ²¡æœ‰è¿™ä¸ªæœºåˆ¶ï¼Œæœºå™¨å¯èƒ½ä¸çŸ¥é“ "it" æŒ‡çš„æ˜¯è¡—é“è¿˜æ˜¯åŠ¨ç‰©ã€‚
                </p>
            </div>

            <p className="mb-4">
              å¯¹äºæ¯ä¸ªè¾“å…¥ Tokenï¼Œæˆ‘ä»¬åˆ›å»ºä¸‰ä¸ªå‘é‡ï¼š
              <strong>Query ($Q$, æŸ¥è¯¢)</strong>, <strong>Key ($K$, é”®)</strong>, å’Œ <strong>Value ($V$, å€¼)</strong>ã€‚
            </p>
            <MathBlock formula="\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V" block />
            
            <CodeBlock code={`# 1. ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (Scaled Dot-Product Attention)
def attention(query, key, value):
    d_k = query.size(-1)
    
    # Q ä¹˜ä»¥ K çš„è½¬ç½® -> å¾—åˆ°åˆ†æ•° (Scores)
    scores = torch.matmul(query, key.transpose(-2, -1))
    
    # é™¤ä»¥ sqrt(d_k) è¿›è¡Œç¼©æ”¾
    scores = scores / math.sqrt(d_k)
    
    # Softmax å½’ä¸€åŒ–ï¼Œå¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
    attn_weights = F.softmax(scores, dim=-1)
    
    # æ¦‚ç‡åŠ æƒ V
    return torch.matmul(attn_weights, value)`} />

            <InteractiveAttention lang="zh" />
            
            <QuizSection 
              lang="zh"
              question={{
                id: 'q3-1',
                question: 'åœ¨å…¬å¼ä¸­ï¼Œä¸ºä»€ä¹ˆè¦é™¤ä»¥ sqrt(d_k)ï¼Ÿ',
                options: [
                  'ä¸ºäº†å‡å°‘è®¡ç®—æ—¶é—´ã€‚',
                  'ä¸ºäº†é˜²æ­¢ç‚¹ç§¯è¿‡å¤§ï¼Œå¯¼è‡´ Softmax è¿›å…¥æ¢¯åº¦æå°çš„åŒºåŸŸï¼ˆæ¢¯åº¦æ¶ˆå¤±ï¼‰ã€‚',
                  'ä¸ºäº†è®©çŸ©é˜µä¹˜æ³•æˆç«‹ã€‚',
                  'è¿™åªæ˜¯ä¸€ä¸ªä»»æ„å¸¸æ•°ã€‚'
                ],
                correctAnswer: 1,
                explanation: 'å¦‚æœç‚¹ç§¯ç»“æœå¾ˆå¤§ï¼ŒSoftmax çš„è¾“å‡ºä¼šæ¥è¿‘ 0 æˆ– 1ï¼Œè¿™æ—¶çš„æ¢¯åº¦éå¸¸å°ï¼Œéš¾ä»¥è®­ç»ƒã€‚ç¼©æ”¾å¯ä»¥é˜²æ­¢è¿™ç§æƒ…å†µã€‚'
              }} 
            />
          </div>
        )
      },
      {
        id: '3-2',
        title: 'å¤šå¤´æ³¨æ„åŠ› (Multi-Head Attention)',
        content: (
          <div>
            <p className="mb-4">
              æ¨¡å‹ä¸æ˜¯åªæ‰§è¡Œä¸€æ¬¡æ³¨æ„åŠ›å‡½æ•°ï¼Œè€Œæ˜¯å¹¶è¡Œæ‰§è¡Œ $h$ æ¬¡ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„çº¿æ€§æŠ•å½±ã€‚è¿™å…è®¸æ¨¡å‹å…³æ³¨ä¸åŒè¡¨ç¤ºå­ç©ºé—´çš„ä¿¡æ¯ã€‚
            </p>
            <div className="p-4 bg-brand-50 rounded-lg text-sm text-brand-900 mb-4">
              <strong>é€šä¿—ç†è§£ï¼š</strong> å°±åƒçœ‹ä¹¦æ—¶ä½¿ç”¨ 8 ç§ä¸åŒé¢œè‰²çš„è§å…‰ç¬”ã€‚
              é»„è‰²ç¬”æ ‡è®°â€œæ—¶é—´â€ï¼Œè“è‰²ç¬”æ ‡è®°â€œäººç‰©â€ï¼Œç»¿è‰²ç¬”æ ‡è®°â€œåŠ¨ä½œâ€ã€‚
              æœ€åæŠŠæ‰€æœ‰æ ‡è®°çš„ä¿¡æ¯æ±‡æ€»ï¼Œä½ å°±å¾—åˆ°äº†æœ€å…¨é¢çš„ç†è§£ã€‚å¦‚æœåªæœ‰ä¸€ç§é¢œè‰²ï¼Œä¿¡æ¯å¯èƒ½ä¼šæ··æ‚ã€‚
            </div>
             <CodeBlock code={`class MultiHeadAttention(nn.Module):
    def forward(self, x):
        batch_size = x.size(0)
        
        # 1. çº¿æ€§æŠ•å½± Q, K, V
        # å°†è¾“å…¥åˆ†å‰²æˆ 'h' ä¸ªå¤´
        Q = self.w_q(x).view(batch_size, -1, self.heads, self.d_k)
        K = self.w_k(x).view(batch_size, -1, self.heads, self.d_k)
        V = self.w_v(x).view(batch_size, -1, self.heads, self.d_k)
        
        # 2. å¯¹æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®— Attention
        # (ç®€åŒ–ä»£ç ï¼Œå®é™…é€šè¿‡çŸ©é˜µè¿ç®—ä¸€æ¬¡æ€§å®Œæˆ)
        out = attention(Q, K, V)
        
        # 3. æ‹¼æ¥å¹¶ç»è¿‡çº¿æ€§å±‚
        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)
        return self.w_o(out)`} />
          </div>
        )
      },
      {
        id: '3-3',
        title: 'ä½ç½®ç¼–ç  (Positional Encoding)',
        difficulty: 'intermediate',
        content: (
          <div>
             <p className="mb-4">
              ç”±äº Transformer æ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œå®ƒæœ¬èº«ä¸çŸ¥é“å•è¯çš„é¡ºåºã€‚æˆ‘ä»¬å¿…é¡»å°†ä½ç½®ä¿¡æ¯æ³¨å…¥åˆ° Embedding ä¸­ã€‚
            </p>
             <div className="bg-yellow-50 p-4 rounded-lg mb-4 border border-yellow-100">
                <h4 className="font-bold text-yellow-800 mb-2">é€šä¿—ç†è§£ï¼š</h4>
                <p className="text-sm">
                    æƒ³è±¡å›¾ä¹¦é¦†æŠŠä¸€æœ¬ä¹¦æ‹†æ•£æˆä¸€å †çº¸ï¼ˆBag of Wordsï¼‰ï¼Œé¡ºåºå…¨ä¹±äº†ã€‚
                    ä½ç½®ç¼–ç å°±åƒæ˜¯åœ¨æ¯ä¸€é¡µçº¸çš„é¡µè„šæ‰“ä¸Šé¡µç ã€‚è¿™æ ·å³ä½¿ä½ åŒæ—¶å¤„ç†æ‰€æœ‰çº¸å¼ ï¼Œä½ ä¹ŸçŸ¥é“å“ªä¸€é¡µåœ¨å‰ï¼Œå“ªä¸€é¡µåœ¨åã€‚
                </p>
            </div>
            <p className="mb-4">
              æˆ‘ä»¬é€šè¿‡ä½¿ç”¨ä¸åŒé¢‘ç‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°ï¼Œå°†ä½ç½®å‘é‡æ·»åŠ åˆ°è¾“å…¥åµŒå…¥ä¸­ã€‚
            </p>
            <MathBlock formula="PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{\\text{model}}})" block />
            <MathBlock formula="PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{\\text{model}}})" block />
            
            <PositionalEncodingViz lang="zh" />
          </div>
        )
      },
      {
        id: '3-4',
        title: 'å‰é¦ˆç¥ç»ç½‘ç»œ (FFN)',
        content: (
          <div>
             <p className="mb-4">
              é™¤äº†æ³¨æ„åŠ›å­å±‚å¤–ï¼Œç¼–ç å™¨å’Œè§£ç å™¨çš„æ¯ä¸€å±‚éƒ½åŒ…å«ä¸€ä¸ªå…¨è¿æ¥çš„å‰é¦ˆç½‘ç»œã€‚è¯¥ç½‘ç»œåˆ†åˆ«ä¸”ç‹¬ç«‹åœ°åº”ç”¨äºæ¯ä¸ªä½ç½®ã€‚
            </p>
            <p className="mb-4">
              å®ƒåŒ…å«ä¸¤ä¸ªçº¿æ€§å˜æ¢ï¼Œä¸­é—´å¤¹ä¸€ä¸ª ReLU æ¿€æ´»å‡½æ•°ã€‚
            </p>
            <MathBlock formula="\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2" block />
            <p className="text-sm text-slate-600 mb-4">
                è¾“å…¥å’Œè¾“å‡ºçš„ç»´åº¦æ˜¯ <MathBlock formula="d_{model} = 512" />ï¼Œè€Œä¸­é—´å±‚çš„ç»´åº¦æ˜¯ <MathBlock formula="d_{ff} = 2048" />ï¼ˆç»´åº¦å…ˆè†¨èƒ€åå‹ç¼©ï¼‰ã€‚
            </p>
             <CodeBlock code={`class PositionwiseFeedForward(nn.Module):
    def __init__(self, d_model, d_ff):
        super(PositionwiseFeedForward, self).__init__()
        # ç»´åº¦è†¨èƒ€ (512 -> 2048)
        self.w_1 = nn.Linear(d_model, d_ff) 
        # ç»´åº¦è¿˜åŸ (2048 -> 512)
        self.w_2 = nn.Linear(d_ff, d_model)

    def forward(self, x):
        # ä¸­é—´ä½¿ç”¨ ReLU æ¿€æ´»
        return self.w_2(F.relu(self.w_1(x)))`} />
          </div>
        )
      },
      {
        id: '3-5',
        title: 'ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›',
        difficulty: 'advanced',
        content: (
           <div>
            <p className="mb-4">
              è¿™æ˜¯ <strong>è§£ç å™¨ (Decoder)</strong> ä¸­ç‰¹æœ‰çš„å±‚ï¼Œå…è®¸è§£ç å™¨æŸ¥çœ‹ <strong>ç¼–ç å™¨ (Encoder)</strong> çš„è¾“å‡ºã€‚
            </p>
            <ul className="list-disc ml-6 space-y-2 mb-4">
                <li><strong>Queries (Q):</strong> æ¥è‡ªè§£ç å™¨çš„å‰ä¸€å±‚ï¼ˆæˆ‘ä»¬å½“å‰æ­£åœ¨ç¿»è¯‘çš„å†…å®¹ï¼‰ã€‚</li>
                <li><strong>Keys (K) & Values (V):</strong> æ¥è‡ªç¼–ç å™¨çš„è¾“å‡ºï¼ˆæºè¯­è¨€å¥å­ï¼‰ã€‚</li>
            </ul>
            <div className="bg-purple-50 p-4 rounded-lg mb-4 border border-purple-100">
                <h4 className="font-bold text-purple-800 mb-2">ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªï¼Ÿ</h4>
                <p className="text-sm">
                   è¿™ä½¿å¾—ç¿»è¯‘ç»“æœä¸åŸå§‹æ–‡æœ¬å¯¹é½ã€‚å¦‚æœè§£ç å™¨è¯•å›¾ç”Ÿæˆâ€œå­¦ç”Ÿâ€è¿™ä¸ªè¯çš„æ³•æ–‡ï¼Œ
                   è¿™ä¸ªæœºåˆ¶å…è®¸å®ƒå»å…³æ³¨ç¼–ç å™¨ç¼–ç çš„è‹±æ–‡å•è¯â€œStudentâ€ã€‚
                </p>
            </div>
           </div>
        )
      },
      {
        id: '3-6',
        title: 'æ®‹å·®è¿æ¥ä¸å½’ä¸€åŒ– (Add & Norm)',
        content: (
            <div>
                <p className="mb-4">
                    æ¯ä¸ªå­å±‚ï¼ˆSelf-Attention, FFNï¼‰çš„è¾“å‡ºè®¡ç®—å…¬å¼ä¸ºï¼š
                </p>
                <MathBlock formula="\\text{LayerNorm}(x + \\text{Sublayer}(x))" block />
                <p className="mb-4">
                    <strong>æ®‹å·®è¿æ¥ (Add)ï¼š</strong> æˆ‘ä»¬å°†è¾“å…¥ $x$ åŠ å›åˆ°è¾“å‡ºä¸Šã€‚è¿™è§£å†³äº†æ·±åº¦ç½‘ç»œä¸­çš„â€œæ¢¯åº¦æ¶ˆå¤±â€é—®é¢˜ã€‚
                    <br />
                    <strong>å±‚å½’ä¸€åŒ– (Norm)ï¼š</strong> å¯¹éšè—å‘é‡è¿›è¡Œæ ‡å‡†åŒ–ï¼Œä»¥ç¨³å®šè®­ç»ƒè¿‡ç¨‹ã€‚
                </p>
            </div>
        )
      }
    ]
  },
  {
    id: 'ch4',
    title: '4. è®­ç»ƒä¸ç»†èŠ‚',
    description: 'å¦‚ä½•è®©æ¨¡å‹å­¦ä¹ ',
    sections: [
      {
        id: '4-1',
        title: 'æ©ç  (Masking)',
        content: (
          <div>
            <h3 className="font-bold text-lg mb-2 text-slate-800">æŠ€æœ¯è§£é‡Š</h3>
            <p className="mb-4">
              <strong>å¡«å……æ©ç  (Padding Mask)ï¼š</strong> å¿½ç•¥è¾“å…¥åºåˆ—ä¸­ä¸ºäº†å¯¹é½é•¿åº¦è€Œå¡«å……çš„ 0 (Padding Tokens)ã€‚
              <br/>
              <strong>å‰ç»æ©ç  (Look-Ahead Mask)ï¼š</strong> ç”¨äºè§£ç å™¨ã€‚åœ¨é¢„æµ‹ç¬¬ $t$ ä¸ªè¯æ—¶ï¼Œå°† $t$ ä¹‹åçš„ä½ç½®çš„æ³¨æ„åŠ›åˆ†æ•°è®¾ä¸º $-\\infty$ï¼ˆè´Ÿæ— ç©·ï¼‰ã€‚
            </p>
            
            <div className="bg-green-50 p-4 rounded-lg border-l-4 border-green-500 mt-4">
                <h4 className="font-bold mb-1 text-green-800">é€šä¿—ç†è§£ï¼š</h4>
                <p className="text-sm text-green-900">
                    è¿™å°±å¥½æ¯”åœ¨åšè‹±è¯­å¡«ç©ºé¢˜ã€‚
                    å¦‚æœä½ åœ¨å¡«ç¬¬ 3 ä¸ªç©ºçš„æ—¶å€™ï¼Œå·çœ‹äº†ç¬¬ 4 ä¸ªç©ºçš„ç­”æ¡ˆï¼Œé‚£ä½ å°±æ²¡æœ‰çœŸæ­£å­¦ä¼šé¢„æµ‹ã€‚
                    <strong>å‰ç»æ©ç </strong>å°±åƒæ˜¯ç”¨ä¸€å¼ çº¸æŠŠåé¢çš„ç­”æ¡ˆæŒ¡ä½ï¼Œå¼ºè¿«æ¨¡å‹åªèƒ½æ ¹æ®å·²çŸ¥çš„ä¸Šæ–‡æ¥æ¨æ–­ä¸‹æ–‡ã€‚
                </p>
            </div>
             <QuizSection 
              lang="zh"
              question={{
                id: 'q4-1',
                question: 'ä¸ºä»€ä¹ˆè§£ç å™¨éœ€è¦å‰ç»æ©ç ï¼Œè€Œç¼–ç å™¨ä¸éœ€è¦ï¼Ÿ',
                options: [
                  'å› ä¸ºç¼–ç å™¨æ˜¯åŒå‘çš„ï¼ˆèƒ½çœ‹åˆ°æ•´ä¸ªå¥å­ï¼‰ï¼Œè€Œè§£ç å™¨æ˜¯é¡ºåºç”Ÿæˆçš„ã€‚',
                  'ç¼–ç å™¨ä¸ä½¿ç”¨è‡ªæ³¨æ„åŠ›ã€‚',
                  'è§£ç å™¨é€Ÿåº¦æ›´å¿«ã€‚',
                  'å¡«å…… Token åªå­˜åœ¨äºè§£ç å™¨ä¸­ã€‚'
                ],
                correctAnswer: 0,
                explanation: 'ç¼–ç å™¨ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªæºå¥å­ã€‚è§£ç å™¨æ˜¯è‡ªå›å½’çš„ï¼Œæ„å‘³ç€å®ƒæ¯æ¬¡ç”Ÿæˆä¸€ä¸ªè¯ï¼Œä¸èƒ½é€šè¿‡â€œå·çœ‹â€åé¢çš„è¯æ¥ä½œå¼Šã€‚'
              }} 
            />
          </div>
        )
      },
      {
        id: '4-2',
        title: 'ä¼˜åŒ–å™¨ä¸æ­£åˆ™åŒ–',
        content: (
          <div>
             <h3 className="font-bold text-lg mb-2 text-slate-800">æŠ€æœ¯è§£é‡Š</h3>
            <p className="mb-4">
              ä½¿ç”¨ <strong>Adam</strong> ä¼˜åŒ–å™¨ ($\beta_1=0.9, \beta_2=0.98$)ã€‚
              å…³é”®åœ¨äºä½¿ç”¨äº† <strong>Warmupï¼ˆçƒ­èº«ï¼‰</strong> ç­–ç•¥ï¼šå­¦ä¹ ç‡åœ¨è®­ç»ƒåˆæœŸçº¿æ€§å¢åŠ ï¼ŒéšåæŒ‰å¹³æ–¹æ ¹å€’æ•°è¡°å‡ã€‚
            </p>
            <MathBlock formula="lrate = d_{\\text{model}}^{-0.5} \\cdot \\min(step\\_num^{-0.5}, step\\_num \\cdot warmup\\_steps^{-1.5})" block />
            
             <h3 className="font-bold text-lg mb-2 mt-6 text-slate-800">æ ‡ç­¾å¹³æ»‘ (Label Smoothing)</h3>
            <p className="mb-4">
              æˆ‘ä»¬ä¸æ˜¯å¼ºè¿«æ¨¡å‹å¯¹æ­£ç¡®ç­”æ¡ˆä¿æŒ 100% çš„è‡ªä¿¡ï¼ˆç›®æ ‡ï¼šæ­£ç¡®è¯ä¸º1.0ï¼Œå…¶ä»–ä¸º0ï¼‰ï¼Œè€Œæ˜¯å¹³æ»‘ç›®æ ‡åˆ†å¸ƒã€‚
              <br/>
              å¦‚æœå¹³æ»‘å‚æ•° <MathBlock formula="\epsilon_{ls} = 0.1" />ï¼Œæ­£ç¡®è¯çš„æ¦‚ç‡å˜ä¸º 0.9ï¼Œå‰©ä½™çš„æ¦‚ç‡åˆ†é…ç»™å…¶ä»–è¯ã€‚
            </p>
            <p className="text-sm text-slate-600 mb-4">
                è¿™è™½ç„¶ä¼šå¢åŠ å›°æƒ‘åº¦ï¼ˆä¸ç¡®å®šæ€§ï¼‰ï¼Œä½†å¯ä»¥é€šè¿‡é˜²æ­¢æ¨¡å‹è¿‡åº¦è‡ªä¿¡å’Œè¿‡æ‹Ÿåˆæ¥æé«˜å‡†ç¡®ç‡å’Œ BLEU åˆ†æ•°ã€‚
            </p>
          </div>
        )
      }
    ]
  },
  {
      id: 'ch5',
      title: '5. æ¨ç†ä¸è§£ç ',
      description: 'ç”Ÿæˆæ–‡æœ¬çš„è¿‡ç¨‹',
      sections: [
          {
              id: '5-1',
              title: 'è‡ªå›å½’ç”Ÿæˆ (Auto-Regressive)',
              content: (
                  <div>
                      <p className="mb-4">
                          åœ¨æ¨ç†ï¼ˆå¦‚ç¿»è¯‘ï¼‰é˜¶æ®µï¼Œæ¨¡å‹æ˜¯ä¸€ä¸ªæ¥ä¸€ä¸ªåœ°ç”Ÿæˆå•è¯ã€‚
                      </p>
                      <ol className="list-decimal ml-6 space-y-2">
                          <li>å°†æºå¥å­è¾“å…¥ç¼–ç å™¨ã€‚</li>
                          <li>ç»™è§£ç å™¨è¾“å…¥ä¸€ä¸ªç‰¹æ®Šçš„ <code>&lt;START&gt;</code> æ ‡è®°ã€‚</li>
                          <li>è§£ç å™¨è¾“å‡ºç¬¬ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚</li>
                          <li>é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„è¯ï¼Œå°†å…¶åŠ å…¥è¾“å…¥ã€‚</li>
                          <li>é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°ç”Ÿæˆ <code>&lt;END&gt;</code> æ ‡è®°ã€‚</li>
                      </ol>
                  </div>
              )
          },
          {
              id: '5-2',
              title: 'è´ªå©ªæœç´¢ä¸é›†æŸæœç´¢',
              content: (
                  <div>
                      <p className="mb-4">
                          <strong>è´ªå©ªæœç´¢ (Greedy Search)ï¼š</strong> æ¯ä¸€æ­¥éƒ½é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„é‚£ä¸ªè¯ã€‚é€Ÿåº¦å¿«ï¼Œä½†å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼ˆç”Ÿæˆçš„å¥å­å¯èƒ½ä¸é€šé¡ºï¼‰ã€‚
                      </p>
                      <p className="mb-4">
                          <strong>é›†æŸæœç´¢ (Beam Search)ï¼š</strong> æ¯ä¸€æ­¥éƒ½ä¿ç•™å‰ $k$ ä¸ªï¼ˆBeam Widthï¼‰æœ€å¯èƒ½çš„å¥å­ç‰‡æ®µã€‚è¿™å…è®¸æ¨¡å‹æ¢ç´¢å¤šç§å¯èƒ½æ€§ï¼Œä»è€Œæ‰¾åˆ°å…¨å±€æ›´å¥½çš„ç¿»è¯‘ã€‚
                      </p>
                      <QuizSection 
                        lang="zh"
                        question={{
                          id: 'q5',
                          question: 'ç›¸æ¯”è´ªå©ªæœç´¢ï¼Œé›†æŸæœç´¢ (Beam Search) çš„ä¸»è¦ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ',
                          options: [
                            'å®ƒæ›´å¿«ã€‚',
                            'å®ƒæ¢ç´¢å¤šæ¡æ½œåœ¨çš„å¥å­è·¯å¾„ï¼Œé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ã€‚',
                            'å®ƒå ç”¨æ›´å°‘å†…å­˜ã€‚',
                            'å®ƒä¸éœ€è¦è§£ç å™¨ã€‚'
                          ],
                          correctAnswer: 1,
                          explanation: 'è´ªå©ªæœç´¢å¯èƒ½ç°åœ¨é€‰äº†ä¸€ä¸ªæ¦‚ç‡æœ€é«˜çš„è¯ï¼Œä½†å¯¼è‡´åé¢æ— è·¯å¯èµ°ã€‚é›†æŸæœç´¢é€šè¿‡ä¿ç•™å¤šä¸ªå€™é€‰ï¼Œè®©â€œçœ¼å…‰æ”¾å¾—æ›´é•¿è¿œâ€ã€‚'
                        }} 
                      />
                  </div>
              )
          }
      ]
  },
  {
      id: 'ch6',
      title: '6. Transformer å®¶æ—',
      description: 'æ¶æ„çš„æ¼”å˜',
      sections: [
          {
              id: '6-1',
              title: 'ä»…ç¼–ç å™¨ (BERT)',
              content: (
                  <div>
                      <h4 className="font-bold text-slate-800">BERT (Bidirectional Encoder Representations from Transformers)</h4>
                      <p className="text-sm mb-2">
                          åªä½¿ç”¨äº† <strong>Encoder</strong> å †å ã€‚
                      </p>
                      <p className="text-sm mb-2">
                          <strong>ç›®æ ‡ï¼š</strong> ç†è§£æ–‡æœ¬ã€‚å®ƒåŒæ—¶æŸ¥çœ‹ä¸Šä¸‹æ–‡ï¼ˆåŒå‘ï¼‰ã€‚
                      </p>
                      <p className="text-sm">
                          <strong>ä»»åŠ¡ï¼š</strong> æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€å‘½åå®ä½“è¯†åˆ«ã€‚
                      </p>
                  </div>
              )
          },
          {
              id: '6-2',
              title: 'ä»…è§£ç å™¨ (GPT)',
              content: (
                  <div>
                      <h4 className="font-bold text-slate-800">GPT (Generative Pre-trained Transformer)</h4>
                      <p className="text-sm mb-2">
                          åªä½¿ç”¨äº† <strong>Decoder</strong> å †å ï¼ˆå¸¦æœ‰æ©ç çš„è‡ªæ³¨æ„åŠ›ï¼‰ã€‚
                      </p>
                      <p className="text-sm mb-2">
                          <strong>ç›®æ ‡ï¼š</strong> ç”Ÿæˆæ–‡æœ¬ã€‚å®ƒæ ¹æ®å‰é¢çš„è¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚
                      </p>
                      <p className="text-sm">
                          <strong>ä»»åŠ¡ï¼š</strong> æ–‡æœ¬ç”Ÿæˆã€èŠå¤©æœºå™¨äººã€ä»£ç è¡¥å…¨ã€‚
                      </p>
                  </div>
              )
          },
          {
              id: '6-3',
              title: 'ç¼–ç å™¨-è§£ç å™¨ (T5 / BART)',
              content: (
                  <div>
                      <h4 className="font-bold text-slate-800">T5 (Text-to-Text Transfer Transformer)</h4>
                      <p className="text-sm mb-2">
                          ä½¿ç”¨äº†å®Œæ•´çš„åŸå§‹æ¶æ„ã€‚
                      </p>
                      <p className="text-sm">
                          <strong>ä»»åŠ¡ï¼š</strong> ç¿»è¯‘ã€æ‘˜è¦ï¼ˆåºåˆ—åˆ°åºåˆ—ä»»åŠ¡ï¼‰ã€‚
                      </p>
                  </div>
              )
          }
      ]
  },
  {
    id: 'ch7',
    title: '7. å®éªŒç»“æœä¸å½±å“',
    description: 'å®ƒçœŸçš„æœ‰æ•ˆå—ï¼Ÿ',
    sections: [
      {
        id: '7-1',
        title: 'å¤šç»´åº¦æ€§èƒ½åˆ†æ',
        content: (
          <div>
             <p className="mb-4">
                Transformer åœ¨ WMT 2014 è‹±å¾·å’Œè‹±æ³•ç¿»è¯‘ä»»åŠ¡ä¸Šå‡è¾¾åˆ°äº†å½“æ—¶çš„ SOTA (State-of-the-art) æ°´å¹³ã€‚
            </p>
            <div className="h-64 w-full mb-6">
              <ResponsiveContainer width="100%" height="100%">
                <BarChart data={perfData} layout="vertical" margin={{ top: 5, right: 30, left: 40, bottom: 5 }}>
                  <CartesianGrid strokeDasharray="3 3" />
                  <XAxis type="number" domain={[20, 30]} />
                  <YAxis dataKey="name" type="category" width={100} style={{fontSize: '12px'}} />
                  <Tooltip />
                  <Legend />
                  <Bar dataKey="bleu" name="BLEU åˆ†æ•°" fill="#0ea5e9" />
                </BarChart>
              </ResponsiveContainer>
              <p className="text-center text-xs text-slate-500 mt-2">BLEU åˆ†æ•°å¯¹æ¯”ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰</p>
            </div>
            
             <div className="grid grid-cols-2 gap-4">
                <div className="p-4 border rounded bg-white">
                    <h4 className="font-bold text-green-600">è®­ç»ƒæˆæœ¬ (Cost)</h4>
                    <p className="text-sm mt-1">
                        Transformer (Base) ä»…æ¶ˆè€—äº† <strong>$3.3 \cdot 10^{18}$</strong> æ¬¡æµ®ç‚¹è¿ç®—ã€‚
                        Transformer (Big) åœ¨ 8 å¼  P100 GPU ä¸Šä»…è®­ç»ƒäº† <strong>3.5 å¤©</strong>ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä¹‹å‰çš„æ¨¡å‹å¾€å¾€éœ€è¦è®­ç»ƒæ•°å‘¨ã€‚
                    </p>
                </div>
                <div className="p-4 border rounded bg-white">
                    <h4 className="font-bold text-purple-600">æ³›åŒ–èƒ½åŠ› (Generalization)</h4>
                    <p className="text-sm mt-1">
                        è®ºæ–‡è¯æ˜äº†æ¨¡å‹å¯ä»¥å¾ˆå¥½åœ°è¿ç§»åˆ°å…¶ä»–ä»»åŠ¡ã€‚ä¾‹å¦‚åœ¨ <strong>è‹±è¯­æˆåˆ†å¥æ³•åˆ†æ (English Constituency Parsing)</strong> ä»»åŠ¡ä¸­ï¼Œå®ƒåœ¨å‡ ä¹æ²¡æœ‰é’ˆå¯¹æ€§è°ƒä¼˜çš„æƒ…å†µä¸‹ä¹Ÿå–å¾—äº†æå¥½çš„æˆç»©ã€‚
                    </p>
                </div>
            </div>
          </div>
        )
      }
    ]
  },
  {
    id: 'ch8',
    title: '8. èµ„æºä¸ä»£ç ',
    description: 'æ·±å…¥ç ”ç©¶',
    sections: [
      {
        id: '8-1',
        title: 'å®˜æ–¹å®ç°ä¸æºç ',
        content: (
          <div>
            <p className="mb-4">
                Transformer çš„åŸå§‹ä»£ç å‘å¸ƒåœ¨ Google çš„ Tensor2Tensor åº“ä¸­ã€‚ä»¥ä¸‹æ˜¯é‡è¦èµ„æºçš„é“¾æ¥ï¼š
            </p>
            <ul className="space-y-4">
                <li>
                    <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noreferrer" className="flex items-center gap-2 p-4 border rounded hover:bg-slate-50 transition group">
                        <span className="text-2xl">ğŸ“„</span>
                        <div>
                            <div className="font-bold text-brand-600 group-hover:underline">é˜…è¯»åŸå§‹è®ºæ–‡ (ArXiv)</div>
                            <div className="text-sm text-slate-500">Attention Is All You Need (Vaswani et al., 2017)</div>
                        </div>
                    </a>
                </li>
                <li>
                    <a href="https://github.com/tensorflow/tensor2tensor" target="_blank" rel="noreferrer" className="flex items-center gap-2 p-4 border rounded hover:bg-slate-50 transition group">
                        <span className="text-2xl">ğŸ’»</span>
                        <div>
                            <div className="font-bold text-slate-800 group-hover:underline">Tensor2Tensor (GitHub)</div>
                            <div className="text-sm text-slate-500">è®ºæ–‡ä½¿ç”¨çš„å®˜æ–¹ TensorFlow å®ç°ã€‚</div>
                        </div>
                    </a>
                </li>
                 <li>
                    <a href="https://github.com/pytorch/fairseq" target="_blank" rel="noreferrer" className="flex items-center gap-2 p-4 border rounded hover:bg-slate-50 transition group">
                        <span className="text-2xl">ğŸ”¥</span>
                        <div>
                            <div className="font-bold text-slate-800 group-hover:underline">PyTorch FairSeq (GitHub)</div>
                            <div className="text-sm text-slate-500">Facebook AI Research æ¨å‡ºçš„åºåˆ—å»ºæ¨¡å·¥å…·åŒ…ï¼ŒåŒ…å«é«˜è´¨é‡çš„ Transformer å®ç°ã€‚</div>
                        </div>
                    </a>
                </li>
            </ul>
          </div>
        )
      }
    ]
  }
];

export const getChapters = (lang: Language) => lang === 'zh' ? chaptersZh : chaptersEn;